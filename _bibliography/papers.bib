@inproceedings{srinivasan2024forward,
	bibtex_show={true},
	title={Forward Learning with Top-Down Feedback: Empirical and Analytical Characterization},
	author={Ravi Francesco Srinivasan and Francesca Mignacco and Martino Sorbaro and Maria Refinetti and Avi Cooper and Gabriel Kreiman and Giorgia Dellaferrera},
	booktitle={The Twelfth International Conference on Learning Representations},
	year={2024},
	abstract = {"Forward-only" algorithms, which train neural networks while avoiding a backward pass, have recently gained attention as a way of solving the biologically unrealistic aspects of backpropagation. Here, we first address compelling challenges related to the "forward-only" rules, which include reducing the performance gap with backpropagation and providing an analytical understanding of their dynamics. To this end, we show that the forward-only algorithm with top-down feedback is well-approximated by an "adaptive-feedback-alignment" algorithm, and we analytically track its performance during learning in a prototype high-dimensional setting. Then, we compare different versions of forward-only algorithms, focusing on the Forward-Forward and PEPITA frameworks, and we show that they share the same learning principles. Overall, our work unveils the connections between three key neuro-inspired learning rules, providing a link between "forward-only" algorithms, i.e., Forward-Forward and PEPITA, and an approximation of backpropagation, i.e., Feedback Alignment.},
	url={https://openreview.net/forum?id=My7lkRNnL9},
	selected={true},
	pdf={https://openreview.net/pdf?id=My7lkRNnL9},
	poster={https://iclr.cc/media/PosterPDFs/ICLR%202024/18804.png?t=1714896864.2536316},
	preview={PEPITA.png},
}

@article {Xiao2023.03.27.534384,
	bibtex_show={true},
	author = {Yuchen Xiao* and Paula S{\'a}nchez L{\'o}pez* and Ruijie Wu* and Ravi Srinivasan and Peng-Hu Wei and Yong-Zhi Shan and Daniel Weisholtz and Garth Rees Cosgrove and Joseph R Madsen and Scellig Stone and Guo-Guang Zhao and Gabriel Kreiman},
	title = {Neurophysiological and computational mechanisms of non-associative and associative memories during complex human behavior},
	elocation-id = {2023.03.27.534384},
	year = {2023},
	doi = {10.1101/2023.03.27.534384},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {The ability to transiently remember what happened where and when is a cornerstone of cognitive function. Forming and recalling working memories depends on detecting novelty, building associations to prior knowledge, and dynamically retrieving context-relevant information. Previous studies have scrutinized the neural machinery for individual components of recognition or associative memory under laboratory conditions, such as recalling elements from arbitrary lists of words or pictures. In this study, we implemented a well-known card- matching game that integrates multiple components of memory formation together in a naturalistic setting to investigate the dynamic neural processes underlying complex natural human memory. We recorded intracranial field potentials from 1,750 depth or subdural electrodes implanted in 20 patients with pharmacologically-intractable epilepsy while they were performing the task. We leveraged generalized linear models to simultaneously assess the relative contribution of neural responses to distinct task components. Neural activity in the gamma frequency band signaled novelty and graded degrees of familiarity, represented the strength and outcome of associative recall, and finally reflected visual feedback on a trial-by-trial basis. We introduce an attractor-based neural network model that provides a plausible first-order approximation to capture the behavioral and neurophysiological observations. The large-scale data and models enable dissociating and at the same time dynamically tracing the different cognitive components during fast, complex, and natural human memory behaviors.},
	journal = {bioRxiv preprint},
	selected={true},
	pdf={https://www.biorxiv.org/content/biorxiv/early/2023/10/02/2023.03.27.534384.full.pdf},
	preview={memory_game.png},
}

@mastersthesis{srinivasan2023hebbian,
  title={Hebbian attractor to model working memory in complex human behavior},
  author={Srinivasan, Ravi},
  year={2023},
  publisher = {Master Thesis},
  school={ETH Zurich},
  pdf={https://klab.tch.harvard.edu/publications/PDFs/gk8154.pdf},
  preview={attractor_WM.png},
  abstract={Working memory is a crucial cognitive function that enables temporary storage, retrieval, and manipulation of information in goal-directed be- havior. While previous research has predominantly dissected these processes in isolation within controlled settings, this study takes a novel approach by examining and modeling the dynamic neural mech- anisms underlying multifaceted working memory during a real-world card-matching game. The participants were presented with covered images arranged in a grid and were instructed to flip two images at that time until all matching pairs were found. During this task the participants needed to keep track of the position and content of the images, allowing the analysis of behavioral data and neural activities. We introduced a Hebbian attractor network to model and characterize the memory dynamics of this complex naturalistic task. We showed that the model was able to accurately predict human behavior, and demonstrated similar patterns of memory decay and reaction times. Moreover, we found qualitative equivalents to patients’ neural signals that encoded novelty and familiarity, as well as signals that predicted correct retrieval from memory of a tile’s pair location. The high tem- poral resolution, extensive spatial sampling, and computational model provide an opportunity to characterize the dynamics of memory in a complex naturalistic task.}
}